{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ajitabh-12-7/Mini-project-1/blob/main/Mini_Project_1_classwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqfNqaJCQeEE"
      },
      "source": [
        "## Importing the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhZUQ78gzw8H"
      },
      "outputs": [],
      "source": [
        "# to read and manipulate the data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option('max_colwidth', None)    # setting column to the maximum column width as per the data\n",
        "\n",
        "# to visualise data\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# to use regular expressions for manipulating text data\n",
        "import re\n",
        "\n",
        "# to load the natural language toolkit\n",
        "# loading the wordnet module that is used in stemming\n",
        "\n",
        "# to remove common stop words\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# to perform stemming\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# to create Bag of Words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# to split data into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# to build a Random Forest model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# to compute metrics to evaluate the model\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# To tune different models\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df= pd.read_csv('/content/Product_Reviews.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "23wuOFYLYUcj",
        "outputId": "f322ccd1-008c-497d-88c4-1aa8ee995f18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Product ID  \\\n",
              "0  AVpe7AsMilAPnD_xQ78G   \n",
              "1  AVpe7AsMilAPnD_xQ78G   \n",
              "2  AVpe7AsMilAPnD_xQ78G   \n",
              "3  AVpe7AsMilAPnD_xQ78G   \n",
              "4  AVpe7AsMilAPnD_xQ78G   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Product Review  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      I initially had trouble deciding between the paperwhite and the voyage because reviews more or less said the same thing: the paperwhite is great, but if you have spending money, go for the voyage.Fortunately, I had friends who owned each, so I ended up buying the paperwhite on this basis: both models now have 300 ppi, so the 80 dollar jump turns out pricey the voyage's page press isn't always sensitive, and if you are fine with a specific setting, you don't need auto light adjustment).It's been a week and I am loving my paperwhite, no regrets! The touch screen is receptive and easy to use, and I keep the light at a specific setting regardless of the time of day. (In any case, it's not hard to change the setting either, as you'll only be changing the light level at a certain time of day, not every now and then while reading).Also glad that I went for the international shipping option with Amazon. Extra expense, but delivery was on time, with tracking, and I didnt need to worry about customs, which I may have if I used a third party shipping service.   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Allow me to preface this with a little history. I am (was) a casual reader who owned a Nook Simple Touch from 2011. I've read the Harry Potter series, Girl with the Dragon Tattoo series, 1984, Brave New World, and a few other key titles. Fair to say my Nook did not get as much use as many others may have gotten from theirs.Fast forward to today. I have had a full week with my new Kindle Paperwhite and I have to admit, I'm in love. Not just with the Kindle, but with reading all over again! Now let me relate this review, love, and reading all back to the Kindle. The investment of 139.00 is in the experience you will receive when you buy a Kindle. You are not simply paying for a screen there is an entire experience included in buying from Amazon.I have been reading The Hunger Games trilogy and shall be moving onto the Divergent series soon after. Here is the thing with the Nook that hindered me for the past 4 years: I was never inspired to pick it up, get it into my hands, and just dive in. There was never that feeling of oh man, reading on this thing is so awesome. However, with my Paperwhite, I now have that feeling! That desire is back and I simply adore my Kindle. If you are considering purchasing one, stop thinking about it simply go for it. After a full week, 3 downloaded books, and a ton of reading, I still have half of my battery left as well.Make yourself happy. Inspire the reader inside of you.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        I am enjoying it so far. Great for reading. Had the original Fire since 2012. The Fire used to make my eyes hurt if I read too long. Haven't experienced that with the Paperwhite yet.   \n",
              "3                                                                                                                                                                                                                                                       I bought one of the first Paperwhites and have been very pleased with it its been a constant companion and I suppose Ive read, on average, a book every three days for the past however many years on it. I wouldnt give it up youd have to pry it from my cold dead fingers.For sundry logistical reasons, Ive also made good use of Amazons Kindle app on my iPhone. No Paperwhite screen, naturally, and all the cool usability that delivers, but it works well and has its own attractions as a companion to the Kindle.Of course, there are aspects of the Paperwhite which I would like to critique. Ah you knew that was coming somewhere, didnt you.As a member of BookBub, I get a daily list of alerts and book deals in my chosen genres. I take on many of them, however, Ive found that, even with the best will in the world, I cant keep up. Some days it seems that for every book I read, Ive bought two. Theres just so much good stuff out there! The accumulative effect of this is that the number of books actually on my Paperwhite has been creeping ever upward for some time. Its now at about 400.With this in mind, Ive noticed that while page-turning has remained exactly the same, just about every other action on the Kindle has become positively glacial. Not just very slow, but so slow you think its malfunctioning. The general consensus appears to be that its to be expected once one has that many books downloaded onto a Kindle, it will begin to behave in a flakey manner. This drives me mad. Amazon states it can hold thousands of books. I believe them. But I figure I would need a second Paperwhite to read while Im waiting for actions to complete on the first one.Read more   \n",
              "4  I have to say upfront - I don't like coroporate, hermetically closed stuff like anything by Apple or in this case, Amazon. I like having devices on which I can put anything I want and use it. But...I was a fairly happy user of a Nook Touch for several years, but couldn't use all its functionalities since I live in Serbia. Then I lost the Nook and since no other devices can actually be fully used in Serbia (buying books with them, using their online capabilities) except the Kindle, and since no one except Amazon ships to Serbia, and since I've actually been a happy Amazon customer since 2005 over friends' accounts and since 2007 through my own, and since the Kindle definitely has the best technology - why not buy itSo I did. What I read in many reviews about the screen/light of the Paperwhite and similar devices was no problem with mine. The light disperses just fine, except a few black blotches (maybe you can see it in the picture) at the bottom of the screen, which are actually shadows of the black plastic casing and thus can't really be avoided. As you can see in the picture without the light - there are no blotches with light out.The Paperwhite's screen is just marvelous at 300 ppi, the touchscreen works just fine, the store works here in Serbia, and in these two days I've been using it, I'm a happy guy.I had to get the hang on how to make sideloaded books behave at least almost like Amazon books, but that's fine. That's the one thing I'd like to see Amazon do in some future upgrades: make the Kindle treat sideloaded books just like the ones bought from them directly, with sharing funcion (quotes and Goodreads) enabled and so on.The size is perfect, it sits very well in the hand, the light doesn't hurt the eyes in the dark (like the light on a tab does)... the packaging was fine, no problems there and what remains to be seen now is the battery life.So far, I can only recommend it.   \n",
              "\n",
              "  Sentiment  \n",
              "0  POSITIVE  \n",
              "1  POSITIVE  \n",
              "2  POSITIVE  \n",
              "3  POSITIVE  \n",
              "4  POSITIVE  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-867da6fc-3369-4c0c-8d54-4ce57218c38a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Product ID</th>\n",
              "      <th>Product Review</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
              "      <td>I initially had trouble deciding between the paperwhite and the voyage because reviews more or less said the same thing: the paperwhite is great, but if you have spending money, go for the voyage.Fortunately, I had friends who owned each, so I ended up buying the paperwhite on this basis: both models now have 300 ppi, so the 80 dollar jump turns out pricey the voyage's page press isn't always sensitive, and if you are fine with a specific setting, you don't need auto light adjustment).It's been a week and I am loving my paperwhite, no regrets! The touch screen is receptive and easy to use, and I keep the light at a specific setting regardless of the time of day. (In any case, it's not hard to change the setting either, as you'll only be changing the light level at a certain time of day, not every now and then while reading).Also glad that I went for the international shipping option with Amazon. Extra expense, but delivery was on time, with tracking, and I didnt need to worry about customs, which I may have if I used a third party shipping service.</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
              "      <td>Allow me to preface this with a little history. I am (was) a casual reader who owned a Nook Simple Touch from 2011. I've read the Harry Potter series, Girl with the Dragon Tattoo series, 1984, Brave New World, and a few other key titles. Fair to say my Nook did not get as much use as many others may have gotten from theirs.Fast forward to today. I have had a full week with my new Kindle Paperwhite and I have to admit, I'm in love. Not just with the Kindle, but with reading all over again! Now let me relate this review, love, and reading all back to the Kindle. The investment of 139.00 is in the experience you will receive when you buy a Kindle. You are not simply paying for a screen there is an entire experience included in buying from Amazon.I have been reading The Hunger Games trilogy and shall be moving onto the Divergent series soon after. Here is the thing with the Nook that hindered me for the past 4 years: I was never inspired to pick it up, get it into my hands, and just dive in. There was never that feeling of oh man, reading on this thing is so awesome. However, with my Paperwhite, I now have that feeling! That desire is back and I simply adore my Kindle. If you are considering purchasing one, stop thinking about it simply go for it. After a full week, 3 downloaded books, and a ton of reading, I still have half of my battery left as well.Make yourself happy. Inspire the reader inside of you.</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
              "      <td>I am enjoying it so far. Great for reading. Had the original Fire since 2012. The Fire used to make my eyes hurt if I read too long. Haven't experienced that with the Paperwhite yet.</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
              "      <td>I bought one of the first Paperwhites and have been very pleased with it its been a constant companion and I suppose Ive read, on average, a book every three days for the past however many years on it. I wouldnt give it up youd have to pry it from my cold dead fingers.For sundry logistical reasons, Ive also made good use of Amazons Kindle app on my iPhone. No Paperwhite screen, naturally, and all the cool usability that delivers, but it works well and has its own attractions as a companion to the Kindle.Of course, there are aspects of the Paperwhite which I would like to critique. Ah you knew that was coming somewhere, didnt you.As a member of BookBub, I get a daily list of alerts and book deals in my chosen genres. I take on many of them, however, Ive found that, even with the best will in the world, I cant keep up. Some days it seems that for every book I read, Ive bought two. Theres just so much good stuff out there! The accumulative effect of this is that the number of books actually on my Paperwhite has been creeping ever upward for some time. Its now at about 400.With this in mind, Ive noticed that while page-turning has remained exactly the same, just about every other action on the Kindle has become positively glacial. Not just very slow, but so slow you think its malfunctioning. The general consensus appears to be that its to be expected once one has that many books downloaded onto a Kindle, it will begin to behave in a flakey manner. This drives me mad. Amazon states it can hold thousands of books. I believe them. But I figure I would need a second Paperwhite to read while Im waiting for actions to complete on the first one.Read more</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
              "      <td>I have to say upfront - I don't like coroporate, hermetically closed stuff like anything by Apple or in this case, Amazon. I like having devices on which I can put anything I want and use it. But...I was a fairly happy user of a Nook Touch for several years, but couldn't use all its functionalities since I live in Serbia. Then I lost the Nook and since no other devices can actually be fully used in Serbia (buying books with them, using their online capabilities) except the Kindle, and since no one except Amazon ships to Serbia, and since I've actually been a happy Amazon customer since 2005 over friends' accounts and since 2007 through my own, and since the Kindle definitely has the best technology - why not buy itSo I did. What I read in many reviews about the screen/light of the Paperwhite and similar devices was no problem with mine. The light disperses just fine, except a few black blotches (maybe you can see it in the picture) at the bottom of the screen, which are actually shadows of the black plastic casing and thus can't really be avoided. As you can see in the picture without the light - there are no blotches with light out.The Paperwhite's screen is just marvelous at 300 ppi, the touchscreen works just fine, the store works here in Serbia, and in these two days I've been using it, I'm a happy guy.I had to get the hang on how to make sideloaded books behave at least almost like Amazon books, but that's fine. That's the one thing I'd like to see Amazon do in some future upgrades: make the Kindle treat sideloaded books just like the ones bought from them directly, with sharing funcion (quotes and Goodreads) enabled and so on.The size is perfect, it sits very well in the hand, the light doesn't hurt the eyes in the dark (like the light on a tab does)... the packaging was fine, no problems there and what remains to be seen now is the battery life.So far, I can only recommend it.</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-867da6fc-3369-4c0c-8d54-4ce57218c38a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-867da6fc-3369-4c0c-8d54-4ce57218c38a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-867da6fc-3369-4c0c-8d54-4ce57218c38a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-86ed6907-fb1e-4977-876e-e92b62c4f872\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-86ed6907-fb1e-4977-876e-e92b62c4f872')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-86ed6907-fb1e-4977-876e-e92b62c4f872 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1007,\n  \"fields\": [\n    {\n      \"column\": \"Product ID\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 66,\n        \"samples\": [\n          \"AVsRinWmQMlgsOJE6zxC\",\n          \"AVzRlqklGV-KLJ3aavB0\",\n          \"AVpe7AsMilAPnD_xQ78G\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Product Review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 908,\n        \"samples\": [\n          \"Originally got the walnut cover with my Oasis. It's very nice, and I would describe it as a suede material. For me, the walnut cover didn't age well. Lots of scratching, which is' necessarily a bad thing, but it also seemed to darken a bit which made it a bit dirty. The merlot cover is also very nice and ages a whole lot better. After some months of use, it looks as good today as when I first bought it. Oasis covers in general are very functional. The 2nd battery in the cover is genius. Since I found I really like reading my Oasis without the cover, I love the easy on/off design of the case.\",\n          \"Bought the Echo Tap, Echo Dot, Phillips Hue Starter Kit and a Honeywell Thermostat for my wife for her birthday.She loves it and so do I!Cant wait to add more automated products to it. No need to have my phone out fumbling through multiple apps to change lighting and temperature, check weather, play music, etc., I just ask Alexa to do it while I continue at what I'm doing.Also, when the Dot and Tap are in the same room, the one that is closest to you typically answers your requests. Nice device that will only improve with time.\",\n          \"I habe the echo and I bought this one fort my son's room but it's not as user friendly. I wish I would've just bought the other one.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"POSITIVE\",\n          \"NEUTRAL\",\n          \"NEGATIVE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fantastic-rebel"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEvz0b9gzw8U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "21f51a72-e5c4-416c-ef99-6502216e157e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'reviews' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-21-303450434.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# creating a copy of the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'reviews' is not defined"
          ]
        }
      ],
      "source": [
        "# creating a copy of the data\n",
        "data = reviews.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvlzvKeqAH-i"
      },
      "source": [
        "## Data Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIH4md8nAL4v"
      },
      "source": [
        "### Checking the first five rows of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bj4-QHJ6BiCS"
      },
      "outputs": [],
      "source": [
        "data.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuWYF7W_AQx_"
      },
      "source": [
        "### Checking the shape of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mcb3m-xKzw8V"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPbc-9HLs2WI"
      },
      "source": [
        "* The dataset has 1007 rows and 3 columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBuO6NvsAT1k"
      },
      "source": [
        "### Checking for Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0XZhWGRBiCV"
      },
      "outputs": [],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i5McJGWBiCV"
      },
      "source": [
        "* There are no missing values in the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZdNFg-5Zmiz"
      },
      "source": [
        "### Checking for duplicate values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gn5VDFNoBiCW"
      },
      "outputs": [],
      "source": [
        "# checking for duplicate values\n",
        "data.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwMJutR2BiCW"
      },
      "source": [
        "* There are 2 duplicate values in the dataset.\n",
        "* We'll drop them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mgEIbCEcurU"
      },
      "outputs": [],
      "source": [
        "# dropping duplicate values\n",
        "data = data.drop_duplicates()\n",
        "\n",
        "data.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "3VQpPykLdgsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUJ_B5KxhU3D"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akIOIRhfbvG8"
      },
      "source": [
        "#### Distribution of sentiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5JSc0t9YI90"
      },
      "outputs": [],
      "source": [
        "sns.countplot(data=data, x=\"Sentiment\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYvGpRqf3ovu"
      },
      "outputs": [],
      "source": [
        "data['Sentiment'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqnuCtG7bvHB"
      },
      "source": [
        "- Majority of the reviews are positive (\\~85%), followed by neutral reviews (8%), and then the positive reviews (\\~7%)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recommended Metrics for this Case:\n",
        "| Metric                               | Why It's Important                                                                  |\n",
        "| ------------------------------------ | ----------------------------------------------------------------------------------- |\n",
        "| **Macro F1-Score**                   | Gives equal importance to all 3 classes regardless of imbalance.                    |\n",
        "| **Per-class Precision & Recall**     | Helps you understand how well the model detects **Neutral** and **Negative** cases. |\n",
        "| **Confusion Matrix**                 | Shows what types of mistakes your model is making.                                  |\n",
        "| *(Optional)* **ROC-AUC (per class)** | Can be helpful if you're using probabilistic outputs.                               |\n"
      ],
      "metadata": {
        "id": "-wB_7p2rlkyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Macro F1 vs Micro F1\n",
        "| Type         | Use When                                              | What It Does                           |\n",
        "| ------------ | ----------------------------------------------------- | -------------------------------------- |\n",
        "| **Macro F1** | Treat all classes equally (class-balanced evaluation) | Averages F1 across all classes         |\n",
        "| **Micro F1** | Use when class sizes vary (class-imbalanced)          | Calculates global counts of TP, FP, FN |\n"
      ],
      "metadata": {
        "id": "-r5cv8iVlzIB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4fvprg6u5fE"
      },
      "source": [
        "## Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJDPhhmvvxJ1"
      },
      "source": [
        "### Removing special characters from the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGR2c6dCvT-Q"
      },
      "outputs": [],
      "source": [
        "# defining a function to remove special characters\n",
        "def remove_special_characters(text):\n",
        "    # Defining the regex pattern to match non-alphanumeric characters\n",
        "    pattern = '[^A-Za-z0-9]+'\n",
        "\n",
        "    # Finding the specified pattern and replacing non-alphanumeric characters with a blank string\n",
        "    new_text = ''.join(re.sub(pattern, ' ', text))\n",
        "\n",
        "    return new_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQv4x8npctFZ"
      },
      "outputs": [],
      "source": [
        "# Applying the function to remove special characters\n",
        "data['cleaned_text'] = data['Product Review'].apply(remove_special_characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCxMsKChvT79"
      },
      "outputs": [],
      "source": [
        "# checking a couple of instances of cleaned data\n",
        "data.loc[0:3, ['Product Review','cleaned_text']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpsuDWLnyJFw"
      },
      "source": [
        "- We can observe that the function removed the special characters and retained the alphabets and numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DftSZK9yQ74"
      },
      "source": [
        "### Lowercasing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLOmMLSLRJT0"
      },
      "outputs": [],
      "source": [
        "# changing the case of the text data to lower case\n",
        "data['cleaned_text'] = data['cleaned_text'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P80CyzprdHH1"
      },
      "outputs": [],
      "source": [
        "# checking a couple of instances of cleaned data\n",
        "data.loc[0:3, ['Product Review','cleaned_text']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF_kFCAxyg6L"
      },
      "source": [
        "- We can observe that all the text has now successfully been converted to lower case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLoWwpxzylZH"
      },
      "source": [
        "### Removing extra whitespace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjJN53m8RWCW"
      },
      "outputs": [],
      "source": [
        "# removing extra whitespaces from the text\n",
        "data['cleaned_text'] = data['cleaned_text'].str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCx3mBXiHRax"
      },
      "outputs": [],
      "source": [
        "# checking a couple of instances of cleaned data\n",
        "data.loc[0:3, ['Product Review','cleaned_text']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwVOVENFz9fJ"
      },
      "source": [
        "### Removing stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddYQH3ef0AUj"
      },
      "source": [
        "* The idea with stop word removal is to **exclude words that appear frequently throughout** all the documents in the corpus.\n",
        "* Pronouns and articles are typically categorized as stop words.\n",
        "* The `NLTK` library has an in-built list of stop words and it can utilize that list to remove the stop words from a dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zof2x5co2X8g"
      },
      "outputs": [],
      "source": [
        "# defining a function to remove stop words using the NLTK library\n",
        "def remove_stopwords(text):\n",
        "    # Split text into separate words\n",
        "    words = text.split()\n",
        "\n",
        "    # Removing English language stopwords\n",
        "    new_text = ' '.join([word for word in words if word not in stopwords.words('english')])\n",
        "\n",
        "    return new_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtmKZqDwfOlt"
      },
      "outputs": [],
      "source": [
        "# Applying the function to remove stop words using the NLTK library\n",
        "data['cleaned_text_without_stopwords'] = data['cleaned_text'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zikSkmBDfsu9"
      },
      "outputs": [],
      "source": [
        "# checking a couple of instances of cleaned data\n",
        "data.loc[0:3,['cleaned_text','cleaned_text_without_stopwords']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0_cueGLW6Vt"
      },
      "source": [
        "* We observe that all the stopwords have been removed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN9S84Sj2om2"
      },
      "source": [
        "### Stemming/Lemmatization\n",
        "| Feature                       | **Stemming**                                              | **Lemmatization**                                                |\n",
        "| ----------------------------- | --------------------------------------------------------- | ---------------------------------------------------------------- |\n",
        "| **Definition**                | Removes suffixes to reduce words to root form             | Converts word to its **dictionary base form (lemma)**            |\n",
        "| **Output Example**            | running  **run**, studies  **studi**                | running  **run**, studies  **study**                       |\n",
        "| **Approach**                  | Rule-based truncation (chops off ends)                    | Dictionary + morphological analysis                              |\n",
        "| **Accuracy**                  | Lower (may produce non-words)                             | Higher (always valid words)                                      |\n",
        "| **Speed**                     | Fast (simpler rules)                                      | Slower (more complex processing)                                 |\n",
        "| **Tool Examples**             | `PorterStemmer`, `SnowballStemmer`                        | `WordNetLemmatizer`, `spaCy`                                     |\n",
        "| **Grammatical Understanding** |  No                                                      |  Yes (considers part-of-speech)                                 |\n",
        "| **Language Dependency**       | Mostly English, rule-based                                | Requires proper linguistic resources                             |\n",
        "| **Use Case**                  | When speed is critical and precision isnt (e.g., search) | When precision matters (e.g., text understanding, summarization) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTNdymFEKnSG"
      },
      "source": [
        "#### Summary:\n",
        "* Stemming: Quick, crude chopping of word ends. Fast but less accurate.\n",
        "* Lemmatization: Smart, linguistic reduction to base form. Slower but more accurate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparision of Stemming, Lemmatization and POS(Part of Speech) Lemmatization"
      ],
      "metadata": {
        "id": "wE5x7hyFDoks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming using NLTK (PorterStemmer)\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "words = [\"running\", \"studies\", \"flies\", \"easily\", \"happiness\"]\n",
        "\n",
        "for word in words:\n",
        "    print(f\"{word}  {stemmer.stem(word)}\")\n"
      ],
      "metadata": {
        "id": "QQ_-OZrsDJnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization using NLTK (WordNetLemmatizer)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet #Downloads the WordNet lexical database.WordNet is adictionary-like database where Words are grouped into sets of synonyms\n",
        "import nltk\n",
        "\n",
        "# Make sure to download WordNet resources if not already done\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4') # Open Multilingual WordNet package -This adds language translations, richer word forms, and improved morphological data to WordNet.\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = [\"running\", \"studies\", \"flies\", \"better\", \"happiness\"]\n",
        "\n",
        "for word in words:\n",
        "    print(f\"{word}  {lemmatizer.lemmatize(word)}\")\n"
      ],
      "metadata": {
        "id": "wyV211QwDJ47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bonus: POS-aware Lemmatization with spaCy\n",
        "import spacy\n",
        "\n",
        "# Load English tokenizer, POS tagger, lemmatizer\n",
        "nlp = spacy.load(\"en_core_web_sm\") #Loads a pretrained English NLP model\n",
        "\n",
        "doc = nlp(\"running studies flies better happiness\")\n",
        "\n",
        "for token in doc:\n",
        "    print(f\"{token.text} ({token.pos_})  {token.lemma_}\")\n"
      ],
      "metadata": {
        "id": "pNz-gjarDea8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urpKFz7C-D55"
      },
      "source": [
        "The Porter Stemmer is one of the widely-used algorithms for stemming, and it shorten words to their root form by removing suffixes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2Nv5egY25SY"
      },
      "outputs": [],
      "source": [
        "# defining a function to perform stemming\n",
        "def apply_porter_stemmer(text):\n",
        "    # Split text into separate words\n",
        "    words = text.split()\n",
        "\n",
        "    # Applying the Porter Stemmer on every word of a message and joining the stemmed words back into a single string\n",
        "    new_text = ' '.join([ps.stem(word) for word in words])\n",
        "\n",
        "    return new_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dR0rx6_2oCzY"
      },
      "outputs": [],
      "source": [
        "# Applying the function to perform stemming\n",
        "data['final_cleaned_text'] = data['cleaned_text_without_stopwords'].apply(apply_porter_stemmer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAc0gMU89KM7"
      },
      "outputs": [],
      "source": [
        "# checking a couple of instances of cleaned data\n",
        "data.loc[0:2,['cleaned_text_without_stopwords','final_cleaned_text']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xR3QtkboLXpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3F-popAzw8p"
      },
      "source": [
        "## Text Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Text vectorization is the process of converting text into numerical format so that machine learning models can understand and work with it.\n",
        "\n",
        "* Since ML models can't work with raw text (like \"cat\", \"apple\", \"good\"), we transform the text into vectors (arrays of numbers) that represent words, sentences, or documents."
      ],
      "metadata": {
        "id": "W6Xff8B8LSyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Common Text Vectorization Methods\n",
        "| Method                                                 | Description                                                          | Library                   |\n",
        "| ------------------------------------------------------ | -------------------------------------------------------------------- | ------------------------- |\n",
        "| **Bag of Words (BoW using CountVectorizer)**                                 | Counts how many times each word appears in the document              | `sklearn`                 |\n",
        "| **TF-IDF (Term FrequencyInverse Document Frequency)** | Adjusts BoW by down-weighting common words                           | `sklearn`                 |\n",
        "| **N-grams**                                            | Captures word combinations like bigrams/trigrams                     | `sklearn`                 |\n",
        "| **HashingVectorizer**                                  | Like BoW, but hashes tokens into fixed dimensions (memory-efficient) | `sklearn`                 |\n",
        "| **Word Embeddings (Word2Vec, GloVe)**                  | Maps words to dense, pretrained vectors with meaning                 | `nltk`, `gensim`, `spacy` |\n"
      ],
      "metadata": {
        "id": "yKfaJblQLekb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Bag of words (BOW using CountVectorizer)"
      ],
      "metadata": {
        "id": "e3IFOP2YU2ip"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6QlX2WYzw8s"
      },
      "outputs": [],
      "source": [
        "# Initializing CountVectorizer with top 1000 words\n",
        "bow_vec = CountVectorizer(max_features = 1000)\n",
        "\n",
        "# Applying CountVectorizer on data\n",
        "data_features_BOW = bow_vec.fit_transform(data['final_cleaned_text'])\n",
        "\n",
        "# Convert the data features to array\n",
        "data_features_BOW = data_features_BOW.toarray()\n",
        "\n",
        "# Shape of the feature vector\n",
        "print(\"Shape of the feature vector\",data_features_BOW.shape)\n",
        "\n",
        "# Getting the 1000 words considered by the BoW model\n",
        "words = bow_vec.get_feature_names_out()\n",
        "\n",
        "print(\"first 10 words\",words[:10])\n",
        "print(\"last 10 words\",words[-10:])\n",
        "\n",
        "# Creating a DataFrame from the data features\n",
        "df_BOW = pd.DataFrame(data_features_BOW, columns=bow_vec.get_feature_names_out())\n",
        "df_BOW.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M3x4zoG_bFM"
      },
      "source": [
        "- From the above dataframe, we can observe that the word *yet* is present only once in the third document, and the word *would* is presented twice in the fourth document."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. TFIDFVectorizer"
      ],
      "metadata": {
        "id": "0pqgH43PU-ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initializing tfidf\n",
        "tfidf = TfidfVectorizer(max_features=1000)\n",
        "\n",
        "# Applying TfidfVectorizer on data\n",
        "data_features_tfidf = tfidf.fit_transform(data['final_cleaned_text'])\n",
        "\n",
        "# Convert the data features to array\n",
        "data_features_tfidf = data_features_tfidf.toarray()\n",
        "\n",
        "# Shape of the feature vector\n",
        "print(\"Shape of the feature vector\",data_features_tfidf.shape)\n",
        "\n",
        "# Getting the 1000 words considered by the BoW model\n",
        "words = tfidf.get_feature_names_out()\n",
        "\n",
        "print(\"first 10 words\",words[:10])\n",
        "print(\"last 10 words\",words[-10:])\n",
        "\n",
        "# Creating a DataFrame from the data features\n",
        "df_tfidf = pd.DataFrame(data_features_tfidf, columns=tfidf.get_feature_names_out())\n",
        "df_tfidf.head()\n"
      ],
      "metadata": {
        "id": "V-VXf1CdSDj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. n-grams"
      ],
      "metadata": {
        "id": "hkBPgJZAVGnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing CountVectorizer with top 1000 words\n",
        "ngram = CountVectorizer(max_features = 1000,ngram_range=(1, 2))\n",
        "\n",
        "# Applying CountVectorizer on data\n",
        "data_features_ngram = ngram.fit_transform(data['final_cleaned_text'])\n",
        "\n",
        "# Convert the data features to array\n",
        "data_features_ngram = data_features_ngram.toarray()\n",
        "\n",
        "# Shape of the feature vector\n",
        "print(\"Shape of the feature vector\",data_features_ngram.shape)\n",
        "\n",
        "# Getting the 1000 words considered by the BoW model\n",
        "words = ngram.get_feature_names_out()\n",
        "\n",
        "print(\"first 10 words\",words[:10])\n",
        "print(\"last 10 words\",words[-10:])\n",
        "\n",
        "# Creating a DataFrame from the data features\n",
        "df_ngram = pd.DataFrame(data_features_ngram, columns=ngram.get_feature_names_out())\n",
        "df_ngram.head()"
      ],
      "metadata": {
        "id": "ipxybmbEVJmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4lwYN5bYmHp"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Create a list of datasets and their labels\n",
        "vectorized_datasets = [\n",
        "    (\"BoW\", df_BOW),\n",
        "    (\"TF-IDF\", df_tfidf),\n",
        "    (\"N-gram\", df_ngram)\n",
        "]\n",
        "\n",
        "# Your target variable\n",
        "y = data['Sentiment']\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "# Loop over each dataset and train both classifiers\n",
        "for name, X in vectorized_datasets:\n",
        "    # Split data (80/20)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n",
        "\n",
        "    # Random Forest\n",
        "    rf_model = RandomForestClassifier(random_state=100)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    rf_preds = rf_model.predict(X_test)\n",
        "    rf_f1 = f1_score(y_test, rf_preds, average='macro')\n",
        "    results.append((f\"RandomForest - {name}\", rf_f1, rf_model, X_test, y_test, rf_preds))\n",
        "\n",
        "    # Multinomial Naive Bayes\n",
        "    nb_model = MultinomialNB()\n",
        "    nb_model.fit(X_train, y_train)\n",
        "    nb_preds = nb_model.predict(X_test)\n",
        "    nb_f1 = f1_score(y_test, nb_preds, average='macro')\n",
        "    results.append((f\"NaiveBayes - {name}\", nb_f1, nb_model, X_test, y_test, nb_preds))\n",
        "\n",
        "# Sort results by F1 score (descending)\n",
        "results.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print all F1 scores\n",
        "print(\"\\n Model Performance (Macro F1-scores):\\n\")\n",
        "for label, f1_score_val, _, _, _, _ in results:\n",
        "    print(f\"{label:30s}: Macro F1 = {f1_score_val:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1MJsXAQNErE3",
        "outputId": "6a18d61b-b7ea-48c8-bc38-5749c409e080",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_BOW' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-245499467.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Create a list of datasets and their labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m vectorized_datasets = [\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0;34m\"BoW\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_BOW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m\"TF-IDF\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m\"N-gram\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_ngram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_BOW' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Best model\n",
        "best_model_label, best_f1, best_model, X_test_best, y_test_best, y_pred_best = results[0]\n",
        "\n",
        "print(f\"\\n Best Model: {best_model_label} (Macro F1 = {best_f1:.4f})\\n\")\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test_best, y_pred_best))\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "cm = confusion_matrix(y_test_best, y_pred_best, labels=best_model.classes_)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=best_model.classes_, yticklabels=best_model.classes_)\n",
        "plt.title(f\"Confusion Matrix: {best_model_label}\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jxHB8BboE2vA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i4cByLTXv6H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH77U4ukBiCe"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aetabA3poIq"
      },
      "source": [
        " * Analyzed the distribution of customer sentiments.\n",
        "\n",
        " * Applied text preprocessing techniques to clean raw review data.\n",
        "\n",
        " * Vectorized the text using 3 different techniques and trained a Random Forest model & Naive Baye's Model\n",
        "\n",
        " * Achieved an macro F1 score of 0.57 on the test dataset.\n",
        "\n",
        " * Future improvements include hyperparameter tuning or trying alternative models for better performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEXtBjG_oYwj"
      },
      "source": [
        "### Recommendations:\n",
        "\n",
        "*   Use model predictions to identify customer concerns and take timely,\n",
        "targeted actionsreducing revenue loss and improving satisfaction.\n",
        "*   Leverage sentiment insights to refine marketing strategies:\n",
        "\n",
        "\n",
        "*   Showcase positive feedback in promotions to strengthen brand image.\n",
        "\n",
        "* Use neutral/negative feedback to guide inventory and operational\n",
        "\n",
        "decisions.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "NJDPhhmvvxJ1",
        "2DftSZK9yQ74",
        "hLoWwpxzylZH"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}